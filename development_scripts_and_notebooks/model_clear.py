# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17myRIHRuD5V3LjfvIfh1aeVEp6HSMCqj
"""

#!c1.8
import pandas as pd
import os
import numpy as np
import pickle
from tqdm import tqdm
import math

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from catboost import CatBoostRegressor
import statistics

#!c1.8
pd.set_option("display.max_columns", None)

#!c1.8

mapping = {
    1: "Туристская 2020 год.xlsx",
    2: "Коптевский бул. 2020 год.xlsx",
    3: "Останкино 0 2020 год.xlsx",
    4: "Глебовская 2020 год.xlsx",
    5: "Спиридоновка ул. 2020 год.xlsx",
    6: "Шаболовка 2020.xlsx",
    7: "Академика Анохина 2020.xlsx",
    8: "Бутлерова 2020.xlsx",
    9: "Пролетарский проспект 2020.xlsx",
    10: "Марьино 2020.xlsx"
}

#!c1.8
data = {}

params = {
    "skiprows": [1],
    "engine": "openpyxl"
}

for station_number, filename in mapping.items():
    path = "data/stations/" + filename
    data[station_number] = pd.read_excel(path, **params)


print(data.keys())

#!c1.8
profiles_dir = "data/ostankino_profile/"

daily_tables = {}

for file in os.scandir(profiles_dir):
    daily_table = pd.read_table(file.path, skiprows=19, decimal=",")
    date = file.name[4:12]
    daily_tables[date] = daily_table

#!c1.8
ost_profile_data = pd.concat(daily_tables)
ost_profile_data["data time"] = pd.to_datetime(ost_profile_data["data time"])
ost_profile_data.drop("Quality", axis=1, inplace = True)
ost_profile_data.rename({
    "data time": "datetime",
    "0": "t_0m",
    "50": "t_50m",
    "100": "t_100m",
    "150": "t_150m",
    "200": "t_200m",
    "250": "t_250m",
    "300": "t_300m",
    "350": "t_350m",
    "400": "t_400m",
    "450": "t_450m",
    "500": "t_500m",
    "550": "t_550m",
    "600": "t_600m",
    "OutsideTemperature": "outside_temperature"
}, axis=1, inplace =True)
ost_profile_data.reset_index(drop=True, inplace=True)
ost_profile_data = ost_profile_data.resample("20min", on="datetime").mean().reset_index()

#!c1.8
ost_253_meteo = pd.read_excel("data/ostankino_meteo.xls", skiprows=2, names=["datetime", "253_wind_direction", "253_wind_speed"])
ost_253_meteo = ost_253_meteo.resample("20min", on="datetime").mean().reset_index()

#!c1.8
ost_data = pd.merge(ost_profile_data, ost_253_meteo, how="inner", on="datetime")


#!c1.8
# Drop empty cols, rename them, cast datetime to datetime, join with city-level Ostankino data
for k, v in data.items():
    v = v.loc[:, [name for name in v.columns if "Unnamed" not in name]]
    v.dropna(axis=1, how="all", inplace=True)
    v.rename({
        "Дата и время": "datetime",
        "CO": "co",
        "NO2": "no2",
        "NO": "no",
        "PM10": "pm10",
        "PM2.5": "pm25",
        "-T-": "temperature",
        "| V |": "wind_speed",
        "_V_": "wind_direction",
        "Давление": "pressure",
        "Влажность": "humidity",
        "Осадки": "precipitation"
    }, axis=1, inplace=True)
    v["datetime"] = pd.to_datetime(v["datetime"])
    v = pd.merge(v, ost_data, how="inner", on="datetime")
    data[k] = v

#!c1.8
#clear data

pollutants = ["co", "no2", "no", "pm10", "pm25"]
weather_params = ["temperature", "wind_speed", "wind_direction", "pressure", "humidity", "precipitation"]
dropped_indexes = {}

for item in pollutants+weather_params:
    dropped_indexes[item] = {}
    for i in range(1,11):
        dropped_indexes[item][i] = []

row_count = 26352

for i in range(row_count):
    if i % 1000 == 0:
        print("LINE",i)

    for param_name in pollutants+weather_params:
        tmp_array = []
        raw_array = []
        for j in range(1, 11):
            row = data[j].loc[i]
            raw_array.append(None)
            if param_name in row:
                #print(j,i,param_name,row)
                if not math.isnan(row[param_name]):
                    tmp_array.append(row[param_name])
                    if not(param_name in pollutants and row[param_name] == 0):
                        raw_array[j-1] = row[param_name]
            else:
                #print("ERR",j,i,param_name)
                pass

        if len(tmp_array)>1:
            res_std = statistics.stdev(tmp_array)
            mean_x = statistics.mean(tmp_array)
        else:
            res_std = 0
            if len(tmp_array)>0:
                mean_x = tmp_array[0]
            else:
                mean_x = None

        #print(i,param_name,res_std,mean_x)
        #print(tmp_array,raw_array)

        i_index = 1

        if not mean_x is None:
            for item_val in raw_array:
                if item_val is None or math.isnan(item_val):
                    dropped_indexes[param_name][i_index].append(i)
                elif abs(mean_x-item_val) > 3*res_std and param_name != 'wind_direction':
                    dropped_indexes[param_name][i_index].append(i)
                    print("RESSTD OUT",param_name,j,i,item_val,mean_x,res_std)
        else:
            print("ALL NONE",param_name,j,i)
            for kk in range(1,11):
                dropped_indexes[param_name][kk].append(i)

            i_index +=1
            #if math.isnan(item_val):


#!c1.8

# Split by pollutant

for k, v in data.items():
    v_parts = {}
    for p in pollutants:
        if p in v.columns:
            cols_to_keep = ["temperature", "wind_speed", "wind_direction",\
                            "pressure", "humidity", "precipitation"] + [p] + list(ost_data.columns)
            v_part = v.loc[:, cols_to_keep]
            v_part.rename({p: "pollutant_concentration"}, axis=1,inplace=True)
            v_parts[p] = v_part
    data[k] = v_parts

#!c1.8


#delete lines from drop_indexes

dropped_count = 0

for item in pollutants:
   for i in range(1,11):
       if item in data[i]:
        data[i][item].drop(index = dropped_indexes[item][i] , inplace = True)
        dropped_count += len(dropped_indexes[item][i])

print("DROPPED",dropped_count)

# Add separate date and time variables
for n, dict_ in data.items():
    for pollutant, table in dict_.items():
        table["month"] = table["datetime"].dt.month
        table["day"] = table["datetime"].dt.day
        table["day_of_week"] = table["datetime"].dt.weekday
        table["hour"] = table["datetime"].dt.hour
        table.index = pd.Index(table.datetime)
        table.drop("datetime", axis=1, inplace=True)
    data[n][pollutant] = table

# Generate historical features
# Use rolling to capture some previuos and next measures
hist_features = ["temperature", "wind_speed", "wind_direction",\
                            "pressure", "humidity", "precipitation", "pollutant_concentration"]
for n, dict_ in data.items():
    for pollutant, table in dict_.items():
        for timeshift in [*range(1, 25)] + [168]:
            for feature in hist_features:
                if feature not in list(table.columns):
                    continue
                col_name = feature + "_prev_" + str(timeshift) + "h"
                if timeshift == 168:
                    window = 9
                else:
                    window_size = 6
                col_value = table[feature].rolling(window=window_size).mean().shift(3*timeshift)
                table[col_name] = col_value
        data[n][pollutant] = table

# Generate forecast features
forecast_features = ["temperature", "wind_speed", "wind_direction",\
                            "pressure", "humidity", "precipitation"]
for n, dict_ in data.items():
    for pollutant, table in dict_.items():
        for timeshift in range(1, 25):
            for feature in forecast_features:
                col_name = feature + "_forecast_" + str(timeshift) + "h"
                col_value = table[feature].rolling(window=6).mean().shift(-3*timeshift)
                table[col_name] = col_value
        data[n][pollutant] = table

# Generate target [pollution in 1…24 hours]
for n, dict_ in data.items():
    for pollutant, table in dict_.items():
        for timeshift in range(1, 25):
            col_name = "target_" + str(timeshift) + "h"
            col_value = table["pollutant_concentration"].shift(-3*timeshift)
            table[col_name] = col_value
        data[n][pollutant] = table

#!c1.8
metrics = {}
f_imp = {}

for station_number, station_data in data.items():
    metrics_by_pollutant = {}
    f_imp_by_pollutant = {}
    for pollutant_name, pollutant_data in station_data.items():
        target_names = [name for name in pollutant_data.columns if "target" in name]
        train_data = pollutant_data.dropna(axis=0, subset=target_names)
        
        if train_data.shape[0] < 10000:
            print(f"Not enough data for {pollutant_name} on station {station_number}:\
            n of rows in the dataset is {train_data.shape}, skipping.")
        
        X = train_data.drop(target_names, axis=1)
        y = train_data.loc[:, target_names]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=33)
        
        catboost_params = {
            "iterations": 100,
            "verbose": 100,
            "learning_rate": 1,
            "depth": 8,
            "loss_function": "MultiRMSE"}
        model =  CatBoostRegressor(**catboost_params)
        model.fit(X_train, y_train)
        
        # Uncomment if you want to save models
        # model.save_model(f"pretrained_models/{station_number}_{pollutant_name}.cbm")
        
        predictions = model.predict(X_test)
        
        predictions[predictions < 0] = 0
        
        r2 = r2_score(y_test, predictions, multioutput="raw_values")
        mse = mean_squared_error(y_test, predictions, multioutput="raw_values")
        rmse = model.get_best_score()
        metrics_by_pollutant[pollutant_name] = pd.DataFrame({"r2": r2, "mse": mse, "rmse": rmse["learn"]["MultiRMSE"]})
        
        f_imp_by_pollutant[pollutant_name] = pd.DataFrame({"features": list(X.columns),
                                                           "importance": model.get_feature_importance()})
    metrics[station_number] = pd.concat(metrics_by_pollutant)
    f_imp[station_number] = pd.concat(f_imp_by_pollutant)

#!c1.8

print(metrics_df)

# Look at the metrics by station
metrics_df.groupby(level=0).mean()

#!c1.8

# Look at the metrics by pollutant
metrics_df.groupby(level=1).mean()

#!c1.8

# Look at the metrics by forecast time [1…24]
metrics_df.groupby(level=2).mean()

# The most important features
pd.concat(f_imp).reset_index().drop("level_2", axis=1).drop_duplicates(subset="features").sort_values("importance", ascending=False).head(40)